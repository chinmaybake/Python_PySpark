{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chinmay Bake Machine Learning Assignment 5B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Clear Statement of the problem to solve***\n",
    "\n",
    "Our objective is to predict origin of wines, either US or Non-US, based on the Description from the given historical data. \n",
    "So the bottomline is that we input the description given in the dataset using various NLP processes, so that they are transformed into a numerical vector or would essentially be our feature variable. Our label, is categorical and has two categories to be precise. Finally, our Machine Learning Model would classify the category of the label variable based on the patterns it has learnt from the historical data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Statement/Comment on the data collection (in terms of what is needed) and scope***\n",
    "\n",
    "We must be having written description against each category of origin of wines. The description for a specific category, for instance US wines, would have certain patterns in its description; perhaps recurring words or phrases. We expect our machine learning model to capture this underlying trend within the description and make predictions based on that. \n",
    "We also need the categories to our target variable to be labelled with certain numeric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Origin|         Description|\n",
      "+------+--------------------+\n",
      "|    US|This tremendous 1...|\n",
      "|    US|Mac Watson honors...|\n",
      "|    US|This spent 20 mon...|\n",
      "|    US|This re-named vin...|\n",
      "|    US|The producer sour...|\n",
      "|    US|From 18-year-old ...|\n",
      "|    US|A standout even i...|\n",
      "|    US|With its sophisti...|\n",
      "|    US|First made in 200...|\n",
      "|    US|This blockbuster,...|\n",
      "|    US|This fresh and li...|\n",
      "|    US|Heitz has made th...|\n",
      "|    US|The apogee of thi...|\n",
      "|    US|San Jose-based pr...|\n",
      "|    US|Bergström has mad...|\n",
      "|    US|Focused and dense...|\n",
      "|    US|Cranberry, baked ...|\n",
      "|    US|This standout Roc...|\n",
      "|    US|Steely and perfum...|\n",
      "|    US|The aromas entice...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('ML_NLP').getOrCreate()\n",
    "main_df=spark.read.csv(r'D:\\ShortWineReviews.csv', inferSchema=True,header=True, sep=',')\n",
    "main_df.show()\n",
    "main_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preliminary decision about the ML algorithm you will use. Justify your decision to use this particular algorithm***\n",
    "\n",
    "Our objective is to predict the categories based on given descriptions. Hence, we need a Classification algorithm. \n",
    "Although, the data we have in our target variable is not labelled, we would perform certain tranformations (StringIndexing)\n",
    "so that each category could get a numerical value. For classification, we could either use Logistic Regression or Random Forests. We could try using Random Forests, as the algorithm is more tolerant towards missing values and presence of outliers within the dataset. As we have not weighed a lot upon outlier analysis and missing value treatment in this specific case, we would use Random Forests for classifying our categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data prep and clean-up. \n",
    "\n",
    "def removePunctuation(column):\n",
    "    return trim(lower(regexp_replace(column, '([^\\s\\w_]|_)+', '')))\n",
    "\n",
    "main_df= main_df.withColumn('Description_NoPunct', removePunctuation(col('Description')))\n",
    "\n",
    "tokenization=Tokenizer(inputCol='Description_NoPunct',outputCol='Tokens')\n",
    "tokenized_main_df=tokenization.transform(main_df)\n",
    "\n",
    "stopword_removal=StopWordsRemover(inputCol='Tokens',outputCol='NoStWords_Tokens')\n",
    "NoStWords_df=stopword_removal.transform(tokenized_main_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Statement/comments on data preparation and clean up. What do you need to do to get data ready for the ML algorithm of your choice.***\n",
    "\n",
    "We perform three main steps on the text data before processing it into numeric format. First we define a set of characeters and strip them of the data as a part of removing punctuations. Later, we seperate each word of a sentence individually to form tokenized data. Finally, words which do not have much significance when text analysing are pulled out of the dataset during the stopwords removal process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conduct Explanatory data analysis (EDA). Comment on data distribution. \n",
    "\n",
    "main_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df=main_df.filter(((main_df.Origin =='US') | (main_df.Origin =='Non-US')))\n",
    "main_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+------+\n",
      "|Origin|         Description| Description_NoPunct|length|\n",
      "+------+--------------------+--------------------+------+\n",
      "|Non-US|Aromas that recal...|aromas that recal...|   305|\n",
      "|Non-US|Fruity and attrac...|fruity and attrac...|   131|\n",
      "|Non-US|A full-bodied win...|a fullbodied wine...|   216|\n",
      "|    US|Dark red cherry a...|dark red cherry a...|   297|\n",
      "|    US|Only two acres of...|only two acres of...|   400|\n",
      "+------+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----------+\n",
      "|Origin|avg(Length)|\n",
      "+------+-----------+\n",
      "|Non-US|   245.1995|\n",
      "|    US|   246.5665|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df=main_df.withColumn('length',length(main_df['Description_NoPunct']))\n",
    "main_df.orderBy(rand()).show(5,True)\n",
    "main_df.groupBy('Origin').agg({'Length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conduct Explanatory data analysis (EDA). Comment on data distribution. ***\n",
    "\n",
    "We first ensure that thhere is consitency in the categorical target varialbe across our dataset. Hence we recount the number of records after filtering the dataset with known categories. We can observe that there isn't much of a difference in the mean length of description for both our categories. This possibly indicates that the machine learning won't have excessive learning scope from a specif category as our data is well distrubuted in terms on the lenght of reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Description_NoPunct: string (nullable = true)\n",
      " |-- Tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- NoStWords_Tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+----------------+\n",
      "|    NoStWords_Tokens|OriginIndexLabel|\n",
      "+--------------------+----------------+\n",
      "|[tremendous, 100,...|             1.0|\n",
      "|[mac, watson, hon...|             1.0|\n",
      "|[spent, 20, month...|             1.0|\n",
      "|[renamed, vineyar...|             1.0|\n",
      "|[producer, source...|             1.0|\n",
      "|[18yearold, vines...|             1.0|\n",
      "|[standout, even, ...|             1.0|\n",
      "|[sophisticated, m...|             1.0|\n",
      "|[first, made, 200...|             1.0|\n",
      "|[blockbuster, pow...|             1.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Transformation \n",
    "\n",
    "NoStWords_df.printSchema()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Origin\", outputCol=\"OriginIndexLabel\")\n",
    "indexed_df = indexer.fit(NoStWords_df).transform(NoStWords_df)\n",
    "indexed_df.select(['NoStWords_Tokens','OriginIndexLabel']).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comment on required data transformation needed to get the data ready for input to the ML algorithm of your choice***\n",
    "\n",
    "As illustrated in our dataset's schema, the target variable is categorical. Hence, we map categrical values of that variable against certain numeric values using the StringIndexer function. The resultant column has a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|            features|OriginIndexLabel|\n",
      "+--------------------+----------------+\n",
      "|(262144,[5358,204...|             1.0|\n",
      "|(262144,[40266,43...|             1.0|\n",
      "|(262144,[15519,23...|             1.0|\n",
      "|(262144,[3189,319...|             1.0|\n",
      "|(262144,[15664,21...|             1.0|\n",
      "|(262144,[32653,35...|             1.0|\n",
      "|(262144,[4176,235...|             1.0|\n",
      "|(262144,[5460,157...|             1.0|\n",
      "|(262144,[13114,31...|             1.0|\n",
      "|(262144,[35120,42...|             1.0|\n",
      "|(262144,[15786,21...|             1.0|\n",
      "|(262144,[17200,26...|             1.0|\n",
      "|(262144,[571,1566...|             1.0|\n",
      "|(262144,[932,3007...|             1.0|\n",
      "|(262144,[4176,546...|             1.0|\n",
      "|(262144,[20,614,2...|             1.0|\n",
      "|(262144,[551,2851...|             1.0|\n",
      "|(262144,[5765,606...|             1.0|\n",
      "|(262144,[14114,14...|             1.0|\n",
      "|(262144,[47075,61...|             1.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF\n",
    "hashing_vector=HashingTF(inputCol='NoStWords_Tokens',outputCol='Features')\n",
    "hashing_df=hashing_vector.transform(indexed_df)\n",
    "\n",
    "tf_idf_vec=IDF(inputCol='Features',outputCol='IDF_Features')\n",
    "tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "\n",
    "model_df=tf_idf_df.select(['OriginIndexLabel','IDF_Features'])\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "idf_df_assembler = VectorAssembler(inputCols=['IDF_Features'],outputCol='IDF_Features_Vec')\n",
    "model_df= idf_df_assembler.transform(model_df)\n",
    "\n",
    "new_model_df=model_df.select(col(\"IDF_Features_Vec\").alias(\"features\"),(\"OriginIndexLabel\"))\n",
    "new_model_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Complete all needed data transformation. Use TF-IDF method of transforming token to their respective numeric values.***\n",
    "\n",
    "We first convert the texts into numeric equivalents using the TF-IDF method and then use VectorAssembler for vectorizing the TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the ML algorithm of your choice. Use an 8-/20 split for creating training and test dataset. \n",
    "\n",
    "training_df,test_df=new_model_df.randomSplit([0.80,0.20])\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf_classifier=RandomForestClassifier(labelCol='OriginIndexLabel',numTrees=50).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8171193935565382\n",
      "Training Precision:  0.8190462721719429\n",
      "Area Under Curve Training:  0.906288709574427\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "training_predictions=rf_classifier.transform(training_df)\n",
    "\n",
    "rf_accuracy=MulticlassClassificationEvaluator(labelCol='OriginIndexLabel',metricName='accuracy').evaluate(training_predictions)\n",
    "print('Training Accuracy: ',rf_accuracy)\n",
    "\n",
    "rf_precision=MulticlassClassificationEvaluator(labelCol='OriginIndexLabel',metricName='weightedPrecision').evaluate(training_predictions)\n",
    "print('Training Precision: ', rf_precision)\n",
    "\n",
    "rf_auc=BinaryClassificationEvaluator(labelCol='OriginIndexLabel').evaluate(training_predictions)\n",
    "print('Area Under Curve Training: ', rf_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train the model using the training dataset. Comment on the overall model fit using evaluation metrics of your chosen ML algorithms. Comment on your findings and draw conclusions about the trained model’s accuracy and worthiness.***\n",
    "\n",
    "Our model yeilds an accuracy of 81% which in turn is satisfactory. We observe a high AUC value for the training data which might indicate that majority of True Positives are classified correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7817745803357314\n",
      "Testing Precision:  0.7831011480520992\n",
      "Area Under Curve Testing:  0.8847100782328577\n"
     ]
    }
   ],
   "source": [
    "rf_predictions=rf_classifier.transform(test_df)\n",
    "\n",
    "\n",
    "test_rf_accuracy=MulticlassClassificationEvaluator(labelCol='OriginIndexLabel',metricName='accuracy').evaluate(rf_predictions)\n",
    "print('Testing Accuracy: ',test_rf_accuracy)\n",
    "\n",
    "test_rf_precision=MulticlassClassificationEvaluator(labelCol='OriginIndexLabel',metricName='weightedPrecision').evaluate(rf_predictions)\n",
    "print('Testing Precision: ', test_rf_precision)\n",
    "\n",
    "test_rf_auc=BinaryClassificationEvaluator(labelCol='OriginIndexLabel').evaluate(rf_predictions)\n",
    "print('Area Under Curve Testing: ', test_rf_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***11.\tTest/Evaluate your trained model with test dataset. Again, evaluate the model’s performance using the evaluation metrics available for your ML algorithm. Again, comment on your findings and draw conclusions about the model’s accuracy and worthiness in terms solving the problem you were trying to solve. ***\n",
    "\n",
    "We observe not much of a difference between the training and testing metrics. This clearly indicates that the model has not overfitted the curve and is not biased. The underlying pattern in the description with NLP pipelines which we have implemented could predict categories of the target variable with fair accuracy and precision. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
